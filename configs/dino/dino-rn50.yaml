# Configuration file for the DINO detector
# Filename convention: 
#   - {detector}-{backbone}.yaml

# Currently based on the config: https://github.com/IDEA-Research/DINO/blob/main/config/DINO/DINO_4scale.py
---
detector: "dino"

params:
  positional_embedding_params:
    # The height and width temperature of the positional embedding equation (attention is all you need);
    # see models.layers.positional for more details
    pe_temperature_h: 40
    pe_temperature_w: 40

    # row/col normalize the poisitonal coordinates [0, 2pi)
    normalize: True

  # parameters for the Hungarian Matcher
  matcher:
    type: "hungarian"
    set_cost_class: 2.0
    set_cost_bbox: 5.0
    set_cost_giou: 2.0
    focal_alpha: 0.25

  # TODO: comment
  loss_weights:
    # TODO: comment
    aux_loss: True
    cls_loss_coef: 1.0
    bbox_loss_coef: 5.0
    giou_loss_coef: 2.0
    iterm_loss_ceof: 1.0

  # TODO: comment
  postprocess:
    num_select: 300
    nms_iou_threshold: -1

  backbone_name: "resnet50"
  backbone_params:
    return_levels: [1, 2, 3]
    backbone_freeze_keywords: TODO see if I need this

  architecture_params:
    # parameters for the detector architecture; section D.4 of the DINO paper gives parameters used
    standard:
      num_feature_levels: 4

      # TODO: Comment
      query_dim: 4

      # TODO: comment
      num_patterns: 0

      # TODO: this might not be used? I could leave it in and just use not implemented error
      random_refpoints_xy: False
      fix_refpoints_hw: -1 # comment the options

    # Two stage refers to how the learned obj queries are initalized to the decoder;
    # for the standard two-stage (Section 3.4):
    #   1. select top K encoder features from the last encoder layer as priors to enhance decoder queries
    #   2. 
    two_stage:
      type: "standard" # ["no", "standard"]
      pat_embed: 0
      add_query_num: 0

      # whether to learn the 
      learn_wh: False
      keep_all_tokens: False
      two_stage_default_hw: 0.05
      bbox_embed_share: False
      class_embed_share: False

    denoising:
      # Number of denoising_queries for an image; will be evenly distributed to each GT object 
      # in the image based on the num_objects for the image w/ the most objects in the batch; therefore 
      # images in the batch w/ less objects will have more denoise queries per GT; additionally, 
      # denoise_number will be multiplied by 2 use positive and negative queries; 
      # e.g., if the image with the higheset num_objects is 6, there will be at least 
      # (100//6)*2 = 32 denoise queries per GT object
      denoise_number: 100

      # NOTE: the bash script overrides this parameter to 1.0, which is different than the config
      #       TODO: the paper uses 0.4 so I should play around with this
      denoise_box_noise_scale: 1.0

      denoise_label_noise_ratio: 0.5 

      # TODO rename and see if I can just use the number of classes in the dataset
      denoise_labelbook_size: 91

    # deformable attention params; also contains some general dino params
    transformer:
      # total dimension & number of heads of the transformer encoder/decoder; hidden_dim
      # will be split across num_heads so hidden_dim % num_heads = 0
      hidden_dim: 256
      num_heads: 8

      # Number of learnable object queries that are input into the decoder; maximum number of object 
      # predictions dino will produce (each a bounding box and class)
      num_queries: 900

      # Number of deformable transformer encoder/decoder layers to use
      num_encoder_layers: 6
      num_decoder_layers: 6
      num_unicoder_layers: 0 # ? not sure what this is

      # dimension of feedforward layer
      ff_dim: 2048

      # dropout value and activation function for the encoder and decoder
      dropout: 0.0
      activation: "relu"

      # Whether to normalize at the end of the TransformerEncoder; the name doesn't really make sense
      pre_norm: False

      # whether to return the intermediate layers from the decoder; default is True and this parameter
      # isn't really even used, it just throws an assert if False 
      return_intermediate_dec: True

      # TODO: Comment
      enc_n_points: 4
      dec_n_points: 4

      use_deformable_box_attn: False

      # TODO: comment
      box_attn_type: "roi_align"

      # Adds a 
      add_channel_attention: False

      # Used to initalize TransformerDecoder attributes in __init__()
      # TODO write a little more about rm_dec_query_scale if needed
      dec_layer_number: null
      rm_dec_query_scale: True
      rm_self_attn_layers: null
      rm_detach: null

      # whether to share the parameters of the prediction heads after each decoder layer;
      dec_pred_bbox_embed_share: True
      dec_pred_class_embed_share: True

      # the type of self-attention to use in the decoder; if "sa"
      # use a standard multiheaded self-attention module, not deformable attn
      decoder_self_attn_type: "sa" # ['sa', 'ca_label', 'ca_content']

      # The order to call the modules in the decoder; this order should not be changed 
      # and it follows the original DETR decoder closely (see detr paper figure 10);
      # stands for ["self-attention", "cross-attention", "feedforward network"]
      decoder_module_seq: ["sa", "ca", "ffn"]

      embed_init_tgt: True

      # this is the look-forward twice mechanism (lft); 
      # False=lft which is what DINO uses and dab-detr uses True for this parameter
      use_detached_boxes_dec_out: False

      # Creates the decoder_query_bbox_pertuber; this is False by default and not used
      # still need to briefly understand the code in case it becomes important
      decoder_layer_noise: False
      dec_noise_params:
        dln_xy_noise: 0.2
        dln_hw_noise: 0.2
