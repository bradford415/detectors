# TODO: update for RT detr
# solver params: optimzer, lr scheduler, etc.
solver:
  # the method for extracting parameters from the model; 
  # see solvers.build.get_optimizer_parameters() for more info
  parameter_strategy: "rtdetrv2" 

  optimizer:
    # The optimizer to use; must be one of the names in the optimizer_map dict
    name: "adamw"

    params:
      # main params for the optimizer; parameters not specified in the seperate param groups will 
      # use this as the default
      params:
        lr: 1.e-4
        betas: [0.9, 0.999]
        weight_decay: 0.0001

      # additional params for backbone
      backbone:
        lr: 1.e-5 # lr for only the backbone

      encoder_decoder:
        # sets the weight decay for the encoder/decoder norm/batch norm layers
        weight_decay: 0.0

  lr_scheduler:
    name: multistep_lr
    step_lr_on: "steps"

    params:
      # after 1000 steps decay the learning rate by lr*gamma
      milestones: [3000] # (1000 steps after the warmup?) TODO might need to change this
      gamma: 0.1
      warmup_t: 2000
      t_in_epochs: False
