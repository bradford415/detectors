# solver params: optimzer, lr scheduler, etc.
solver:

  # Keeps a running exponentially moving average model used during 
  # evaluation
  use_ema: True
  ema_params:
    decay: 0.9999
    warmups: 2000

  # the method for extracting parameters from the model; 
  # see solvers.build.get_optimizer_parameters() for more info
  parameter_strategy: "rtdetrv2" 

  optimizer:
    # The optimizer to use; must be one of the names in the optimizer_map dict
    name: "adamw"

    params:
      # main params for the optimizer; parameters not specified in the seperate param groups will 
      # use this as the default
      params:
        lr: 1.e-4
        betas: [0.9, 0.999]
        weight_decay: 0.0001

      # additional params for backbone
      backbone:
        lr: 1.e-5 # lr for only the backbone

      encoder_decoder:
        # sets the weight decay for the encoder/decoder norm/batch norm layers
        weight_decay: 0.0

# NOTE: RTDETRV2 steps here but it's step to 1000 which would be 1000 epochs,
#       it has a seperate lr scheduler for the warmup which is in train_one_epcoh
#       so it steps on steps; from the code, it looks like it just warms up to the base
#       lr rate and stays there sense it will never hit 1000 epochs to drop; therefore
#       if I set milestones to 300 million it should warmup then never drop the LR
  lr_scheduler:
    name: multistep_lr
    step_lr_on: "steps"

    params:
      # after 1000 steps decay the learning rate by lr*gamma
      milestones: [300000000] # (1000 steps after the warmup?) TODO might need to change this
      gamma: 0.1
      warmup_t: 2000
      t_in_epochs: False # timesteps are in steps
