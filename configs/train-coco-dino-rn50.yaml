# Base config file which stores the default parameters which apply to all model configurations
---

# Whether to run in development/debugging mode; this only uses a few samples in in the train/val dataset
# to quickly run the code as well as sets num_workers=0
dev_mode: True

# Base directory for output files; do not change this 
output_dir: "output/train"

# Experiment name; this will be used for the output directory; use "development" when developing
exp_name: "development"

# Name of dataset; must be one of the names in the dataset_map dict
dataset_name: "CocoDetection"

# Log the train progress every n steps
log_train_steps: 20

# Parameters for the dataset class
dataset:

  # Path to the root of the dataset; detects which path to use based on device
  root: "/mnt/d/datasets/coco" # windows path
  root_mac: "/Users/bsele/datasets/coco" # mac path

  # max_class_id + 1 in the coco file; see data.coco_ds.CocoDetection.num_classes for more info;
  # NOTE: the max_id of the "categories" key in the coco annotation file is 90 (so 90 + 1 = 91)
  num_classes: 91

  # Number of CPU processes the next sample in the dataset; use 0 to only use the main process
  num_workers: 4

train:
  # batch_size per gpu; the total batch_size will be batch_size * num_gpus if using DDP
  # so the number of grad_accum_steps = batch_szie * num gpus / effective_batch_size
  batch_size: 1

  # dino uses a bs of 2 per gpu and uses 8 gpus ->  2 *8 = 16 (Table 5.)
  effective_batch_size: 16  # batch_size * num_gpus (processes) * gradient_accumulation_steps

  validation_batch_size: 32
  epochs: 1

  # max_norm for gradient clipping; ensures the magnitude of all parameters <= max_norm
  max_norm: 0.1

  # Number of epochs to checkpoint after; use 'null' to turn off checkpointing
  ckpt_epochs: 1

  # Path of weights file (.pt) to resume training; 
  # use `null` to train a new model from scratch
  checkpoint_path:

  # Path to pretrained backbone weights to use; typically `checkpoint_path` should be `null` if this parameter is not `null`;
  # this parameter is really only used to train a full detector from scratch but with a pretrained backbone; typically pretrained on ImageNet
  backbone_weights:


# solver params: optimzer, lr scheduler, etc.
solver:
  # the method for extracting parameters from the model; see solvers.build.get_optimizer_parameters() for more info
  parameter_strategy: "separate_backbone" # use a different lr for the backbone parameters

  optimizer:
    # The optimizer to use; must be one of the names in the optimizer_map dict
    name: "adamw"

    # additional params
    backbone_lr: 1.e-5 # lr for only the backbone

    # params for the optimizer
    params:
      lr: 1.e-4
      weight_decay: 0.0001

  lr_scheduler:
    name: "step_lr"
    step_lr_on: "epochs" # step the lr after n "epochs" or "steps"
    
    # parameters for the LR scheduler
    params:
      step_size: 11  # step every n
      gamma: 0.1 # multiplicative factor to reduce the lr by

# GPU parameters
cuda:
  # List of GPU devices to use
  gpus: [0]

  # the communications library to use for distributed training; default for cuda should be 
  # "nccl" prounounced "nickel"
  backend: "nccl"


# Reproducibility information
reproducibility:
  seed: 42

